---
layout:     post
title:      iOS 界面渲染与优化
subtitle:    iOS 界面渲染与优化
date:       2020-03-23
author:     LML
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
	- 渲染
	- UI主线程
---


# 前言

本文主要介绍下吗几点知识 

+ iOS 界面渲染流程  
+ 离屏渲染那些事
	+ 什么是离屏渲染？
	+ 哪些操作导致离屏渲染？如何优化
+ 为什么UI必须在主线程进行

# iOS 界面渲染流程
## Runloop 与绘图循环

UIApplication在主线程所初始化的Runloop我们称为Main Runloop，它负责处理app存活期间的大部分事件，如用户交互等，它一直处于不断处理事件和休眠的循环之中，以确保能尽快的将用户事件传递给GPU进行渲染，使用户行为能够得到响应，画面之所以能够得到不断刷新也是因为Main Runloop在驱动着。  
而每一个view的变化的修改并不是立刻变化，相反的会在当前run loop的结束的时候统一进行重绘，这样设计的目的是为了能够在一个runloop里面处理好所有需要变化的view，包括resize、hide、reposition等等，所有view的改变都能在同一时间生效，这样能够更高效的处理绘制，这个机制被称为绘图循环（View Drawing Cycle)。


## 界面渲染流程
### 流程
![](https://pic.downk.cc/item/5ed5f953c2a9a83be560a5d7.jpg)

+ UIKit: 包含各种控件，负责对用户操作事件的响应，本身并不提供渲染的能力
+ Core Animation: 负责所有视图的绘制、显示与动画效果
+ OpenGL ES: 提供2D与3D渲染服务，GPU渲染
+ Core Graphics: 提供2D渲染服务，CPU渲染，注意这个CPU渲染之后还是需要GPU的，但是不需要在渲染，直接交的是bitmap
+ Graphics Hardware: 最底层的GraphicsHardWare是图形硬件（帧缓冲区、视频控制器）
![](https://pic.downk.cc/item/5ed603d7c2a9a83be56da499.jpg)
![](https://pic.downk.cc/item/5ed5f918c2a9a83be5604d88.jpg)

具体分为四个步骤：  

+ Commit Transaction:
	+ Layout: 构建视图布局如addSubview等操作
	+ Display: 重载drawRect:进行时图绘制，该步骤使用CPU与内存
	+ Prepare: 主要处理图像的解码与格式转换等操作
	+ Commit: 将Layer递归打包并发送到Render Server
+ Render Server:
	+ 负责渲染工作，会解析上一步Commit Transaction中提交的信息并反序列化成渲染树（render tree)，随后根据layer的各种属性生成绘制指令，并在下一次VSync信号到来时调用OpenGL进行渲染。
+ GPU:
	+ GPU会等待显示器的VSync信号发出后才进行OpenGL渲染管线，将3D几何数据转化成2D的像素图像和光栅处理，随后进行新的一帧的渲染，并将其输出到缓冲区。
	+ GPU根据layer渲染出bitsmaps直接放到到缓冲区
+ Dispaly:
	+ 从缓冲区中取出画面，并输出到屏幕上。

![](https://pic.downk.cc/item/5ed5fac3c2a9a83be562828c.jpg)

### 屏幕成像原理
由于单缓冲区效率较低（读的时候就不能写了 写的时候就不能读了 这样就存在**GPU和视频控制器相互等待**的问题 所以效率低了），一般会引入双缓冲区（F1和F2），GPU渲染一帧后放入F1，让视频控制器读取，下一帧渲染后放入F2，并将视频控制器的指针指向F2，从而实现交替写入和读取，提高效率。  
但是在双缓冲区下，如果GPU在视频控制器读取完一帧前就放入下一帧，势必导致帧错乱，用户层面则会看到画面撕裂，因此需要有一个读取完成的信号，**GPU等待这个信号后，才进行新的一帧的更新**。由此可解决同步问题，类似生产者消费者模式。  
要在屏幕上显示图像，需要**CPU和GPU**一起协作，CPU计算好显示的内容提交到GPU，GPU渲染完成后将结果放到帧缓存区，随后**视频控制器会按照 VSync 信号逐行读取帧缓冲区的数据，经过可能的数模转换传递给显示器显示**。  
猜测CPU和GPU直接有个队列，CPU的速度可能比GPU快？？？？
？？？缓冲区还没放完，会被显示吗？

### CPU 与GPU

+ CPU 消耗型任务
	+ 布局计算：计算出所有图层的布局信息就会消耗一部分时间。因此我们应该尽量提前计算好布局信息，然后在合适的时机调整对应的属性
	+  对象创建：过程伴随着内存分配、属性设置、甚至还有读取文件等操作，比较消耗 CPU 资源。尽量用轻量的对象代替重量的对象，可以对性能有所优化。比如 CALayer 比 UIView 要轻量许多，如果视图元素不需要响应触摸事件，用 CALayer 会更加合适。
	+  Autolayout
	+  文本计算：例如 UITableView 中，heightForRowAtIndexPath
	+  图像的绘制：Core Graphics
	+  图片的解码
+  GPU消耗型任务
	+ 接收提交的纹理（Texture）和顶点描述（三角形），应用变换（transform）、混合并渲染，然后输出到屏幕上。宽泛的说，【大多数 CALayer 的属性都是用 GPU 来绘制】。
	+ 视图以及图层的混合  

	
# 离屏渲染
## 什么是离屏渲染
如果要在显示屏上显示内容，我们至少需要一块与屏幕像素数据量一样大的frame buffer，作为像素数据存储区域，而这也是GPU存储渲染结果的地方。如果有时因为面临一些限制，无法把渲染结果直接写入frame buffer，而是先暂存在另外的内存区域，之后再写入frame buffer，那么这个过程被称之为离屏渲染。
### CPU 离屏渲染
如果我们在UIView中实现了drawRect方法，就算它的函数体内部实际没有代码，系统也会为这个view申请一块内存区域，等待CoreGraphics可能的绘画操作。  
对于类似这种“新开一块CGContext来画图“的操作，有很多文章和视频也称之为“离屏渲染”（因为像素数据是暂时存入了CGContext，而不是直接到了frame buffer）。进一步来说，其实所有CPU进行的光栅化操作（如文字渲染、图片解码），都**无法直接绘制到由GPU掌管的frame buffer，只能暂时先放在另一块内存之中，说起来都属于“离屏渲染”。**
### GPU离屏渲染
有些没有预合成之前不能直接在屏幕中绘制，不得不开辟一块独立于frame buffer的空白内存，先把容器以及其所有子layer依次画好，再把结果画到frame buffer中。这就是GPU的离屏渲染
## 为什么离屏渲染代价高
离屏渲染是指图层在被显示之前，GPU在当前屏幕缓冲区以外新开辟一个缓冲区进行渲染操作。离屏渲染耗时是发生在离屏这个动作上面，而不是渲染。为什么离屏这么耗时？原因主要有创建缓冲区和上下文切换。创建新的缓冲区代价都不算大，付出最大代价的是上下文切换。

【上下文切换】
不管是在GPU渲染过程中，还是一直所熟悉的进程切换，上下文切换在哪里都是一个相当耗时的操作。首先我要保存当前屏幕渲染环境，然后切换到一个新的绘制环境，申请绘制资源，初始化环境，然后开始一个绘制，绘制完毕后销毁这个绘制环境，如需要切换到On-Screen Rendering或者再开始一个新的离屏渲染重复之前的操作。

## 为什么会产生离屏渲染
+ 在VSync(垂直脉冲)信号作用下，视频控制器每隔16.67ms就会去帧缓冲区(当前屏幕缓冲区)读取渲染后的数据；但是有些效果被认为不能直接呈现于屏幕前，而需要在别的地方做额外的处理，进行预合成。
	+ 比如图层属性的混合体再没有预合成之前不能直接在屏幕中绘制，所以就需要屏幕外渲染
+ 有些视图渲染后的纹理需要被多次复用，但屏幕内的渲染缓冲区是实时更新的，所以需要通过开辟屏幕外的渲染缓冲区，将视图的内容渲染成纹理并缓存，然后再需要的时候在调入屏幕缓冲区，可以避免多次渲染的开销。
## 如何检测离屏渲染
模拟器在工作栏上面的Debug -> Color Off-Screen Rendered
真机在工作栏上面的Debug -> View Debugging -> Rendering -> Color Off-Screen Rendered Yellow
## 怎么优化
### 哪些行为会产生离屏渲染
+ cornerRadius+clipsToBounds
	 
+ shadow
	+ shadow，其原因在于，虽然layer本身是一块矩形区域，但是阴影默认是作用在其中”非透明区域“的，而且需要显示在所有layer内容的下方，因此根据画家算法必须被渲染在先。但矛盾在于此时阴影的本体（layer和其子layer）都还没有被组合到一起，怎么可能在第一步就画出只有完成最后一步之后才能知道的形状呢？这样一来又只能另外申请一块内存，把本体内容都先画好，再根据渲染结果的形状，添加阴影到frame buffer，最后把内容画上去（这只是我的猜测，实际情况可能更复杂）。不过如果我们能够预先告诉CoreAnimation（通过shadowPath属性）阴影的几何形状，那么阴影当然可以先被独立渲染出来，不需要依赖layer本体，也就不再需要离屏渲染了 
+ group opacity
+ mask
+ UIBlurEffect
### 怎么优化
+ cornerRadius+clipsToBounds

```  
   UIButton *btn = [[UIButton alloc]initWithFrame:CGRectMake(130, 330, 100, 100)];  
    [btn setBackgroundColor:[UIColor colorWithRed:(226.0 / 255.0) green:(113.0 / 255.0) blue:(19.0 / 255.0) alpha:1]];  
    [backgroundImageView addSubview:btn];  
    //绘制曲线路径  
    UIBezierPath *maskPath = [UIBezierPath bezierPathWithRoundedRect:btn.bounds   byRoundingCorners:UIRectCornerAllCorners cornerRadii:btn.bounds.size];  
    CAShapeLayer *maskLayer = [[CAShapeLayer alloc]init];  
    //设置大小  
    maskLayer.frame = btn.bounds;  
    //设置图形样子  
    maskLayer.path = maskPath.CGPath;  
    btn.layer.mask = maskLayer;  
   
```   
 
    
    
    
+ shadow
+ 	
+ group opacity
+ mask
+ UIBlurEffect
于界面流畅如果想要深层探索可以看 YYKit作者 写的文章iOS 保持界面流畅的技巧 。该文章从屏幕显示图像的原理，到改进的方案都有详细介绍。

# 为什么必须在主线程操作UI
+ UIKit并不是一个 线程安全 的类，UI操作涉及到渲染访问各种View对象的属性，如果异步操作下会存在读写问题，而为其加锁则会耗费大量资源并拖慢运行速度。
+ 与渲染原理相关
	+  一次Runloop完结 -> Core Animation提交渲染树CA::render::commit 如果多线程，那么渲染频率加快，影响性能
	+  而在渲染方面由于图像的渲染需要以60帧的刷新率在屏幕上 同时 更新，在非主线程异步化的情况下无法确定这个处理过程能够实现同步更新。
+  因为整个程序的起点UIApplication是在主线程进行初始化，所有的用户事件都是在主线程上进行传递（如点击、拖动），所以view只能在主线程上才能对事件进行响应。异步线程容易造成用户操作和相应不同步

# 参考
[](https://juejin.im/post/5c406d97e51d4552475fe178)
[](https://www.jianshu.com/p/57e2ec17585b)
[](https://zhuanlan.zhihu.com/p/72653360)
[](https://www.jianshu.com/p/30f93a9f9540)
	
	
	






 

```   
pthread_mutex_lock:
atomic_dec(pthread_mutex_t.value);
if(pthread_mutex_t.value!=0)
   futex(WAIT)
else
   success

pthread_mutex_unlock:
atomic_inc(pthread_mutex_t.value);
if(pthread_mutex_t.value!=1) 
   futex(WAKEUP) 
else
   success
```  




